version: '3.8'

services:
  traefik:
    container_name: traefik
    profiles: ["test", "full"]
    image: traefik:v3.2.3
    command:
      - "--api.dashboard=true"
      - "--api.insecure=false"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.websecure.address=:443"
      - "--providers.file.directory=/etc/traefik/config"
      - "--providers.file.watch=true"
      - "--log.level=DEBUG"
      # TLS Configuration
      - "--entrypoints.websecure.http.tls=true"
      # Access Logs
      - "--accesslog=true"
      - "--accesslog.filepath=/var/log/traefik/access.log"
      - "--accesslog.format=json"
      - "--accesslog.fields.headers.defaultmode=keep"
      - "--accesslog.fields.headers.names.User-Agent=keep"
      - "--accesslog.fields.headers.names.Authorization=keep"
      - "--accesslog.fields.headers.names.Content-Type=keep"
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dashboard.rule=Host(`demo.ai.qylis.com`)"
      - "traefik.http.routers.dashboard.service=api@internal"
      - "traefik.http.routers.dashboard.entrypoints=websecure"
      #- "traefik.http.routers.dashboard.middlewares=ip-filter@file"
      - "traefik.http.routers.dashboard.middlewares=admin-auth,ip-filter@file"
      # sudo apt-get install apache2-utils ;htpasswd -nb admin Qylis_09_rfv_321|sed -e 's/\$/$$/g'  ==> admin:$$apr1$$Jnxi624s$$FgiRkmvN8PgEwD5U/J4cX/ 
      - "traefik.http.middlewares.admin-auth.basicauth.users=admin:$$apr1$$Jnxi624s$$FgiRkmvN8PgEwD5U/J4cX/"
      - "traefik.http.routers.dashboard.tls=true"
    ports:
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik:/etc/traefik
      - ./certs:/certs:ro
      - ./logs:/var/log/traefik



  ui:
    container_name: ui
    image: qylisdm.gvniot.net/aiboxuin:vn
    profiles: ["test", "full", "iam"]
    environment:
      - REGISTRY_USER=${REGISTRY_USER}
      - REGISTRY_PASSWORD=${REGISTRY_PASSWORD}
    #volumes:
    #  - ./nginx/ui:/usr/share/nginx/html
    expose:
      - "5173"
    ports:
      - "5173:5173"
      - "80:80"
#    environment:
#      AI_BOX_BASE_URL: ${AI_BOX_BASE_URL}
    labels:
      # UI Routes (Priority 1)
      - "traefik.enable=true"
      - "traefik.http.routers.ui.rule=Host(`${UI_DOMAIN}`) && !PathPrefix(`/api`)"
      - "traefik.http.routers.ui.priority=1"
      - "traefik.http.services.ui-frontend.loadbalancer.server.port=80"
      - "traefik.http.routers.ui.service=ui-frontend"
      - "traefik.http.routers.ui.tls=true"
      - "traefik.http.routers.ui.entrypoints=websecure"
      - "traefik.http.routers.ui.middlewares=ip-filter@file"

      # API Routes (Priority 2)
      - "traefik.http.routers.ui-api.rule=Host(`${UI_DOMAIN}`) && PathPrefix(`/api`)"
      - "traefik.http.routers.ui-api.priority=2"
      - "traefik.http.routers.ui-api.service=core-service"
      - "traefik.http.services.core-service.loadbalancer.server.url=http://core:8080"
      - "traefik.http.routers.ui-api.tls=true"
      - "traefik.http.routers.ui-api.entrypoints=websecure"
      - "traefik.http.routers.ui-api.middlewares=ip-filter@file"

    depends_on:
      - core


  postgres:
    # docker compose --profile dev_R up   # remote-registry aibox-image used in docker-compose
    # docker compose --profile dev_R up   # local-registry  aibox-image used in docker-compose
    profiles: ["full", "db", "iam"]  # This service will only run when "dev" or "it" profile is enabled
    image: postgres:16-alpine
    container_name: db
    environment:
      POSTGRES_USER: pguser
      POSTGRES_PASSWORD: pgpass
      POSTGRES_DB: aibox
      client_min_messages: NOTICE    # Shows NOTICE and above
      log_min_messages: NOTICE       # For log file
      log_statement: 'all'           # Logs all SQL statements
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U pguser -d aibox"]
      interval: 5s
      timeout: 5s
      retries: 5
    


  pgadmin:
    profiles: ["full2"]
    image: dpage/pgadmin4
    container_name: db_admin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    volumes:
      - pgadmin-data:/var/lib/pgadmin
    ports:
      - "5050:80"
    depends_on:
      - postgres
    



  core:
    container_name: core
    image: qylisdm.gvniot.net/aibox:vm
    profiles: ["full", "iam"]
    environment:
      - REGISTRY_USER=${REGISTRY_USER}
      - REGISTRY_PASSWORD=${REGISTRY_PASSWORD}
    volumes:
      #- ./nginx/core:/usr/share/nginx/html
      - ./config:/app/config        
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.core.rule=Host(`${CORE_DOMAIN}`)"
      - "traefik.http.services.core-service.loadbalancer.server.port=8080"
      - "traefik.http.routers.core.service=core-service"
      - "traefik.http.routers.core.tls=true"
      - "traefik.http.routers.core.entrypoints=websecure"
      - "traefik.http.routers.core.middlewares=ip-filter@file"
    
    depends_on:
      - minio
      - postgres
      - kafka
      - nats


  apps:
    container_name: apps
    image: qylisdm.gvniot.net/restsvc:v1
    profiles: ["full", "iam"]
    environment:
      - REGISTRY_USER=${REGISTRY_USER}
      - REGISTRY_PASSWORD=${REGISTRY_PASSWORD}
    volumes:
      #- ./nginx/core:/usr/share/nginx/html
      - ./config:/app/config        
    labels:
      - "traefik.enable=true"
      #- "traefik.http.routers.apps.rule=Host(`${APPS_DOMAIN}`) && PathPrefix(`/exec`)"
      - "traefik.http.routers.apps.rule=Host(`${APPS_DOMAIN}`)"
      - "traefik.http.services.apps-service.loadbalancer.server.port=8080"
      - "traefik.http.routers.apps.service=apps-service"
      - "traefik.http.routers.apps.tls=true"
      - "traefik.http.routers.apps.entrypoints=websecure"
      #- "traefik.http.routers.apps.middlewares=apps-debug,ip-filter@file,apps-stripprefix,apps-addprefix"
      - "traefik.http.routers.apps.middlewares=ip-filter@file"
      #- "traefik.http.middlewares.apps-stripprefix.stripprefix.prefixes=/exec"
      #- "traefik.http.middlewares.apps-addprefix.addprefix.prefix=/api/v1/exec"
      #- "traefik.http.middlewares.apps-debug.debugHeaders.verbose=true"
    ports:
      - "8081:8080"
    
    depends_on:
      - postgres
        


  infer:
    container_name: infer
    profiles: ["full"]
    image: "nvcr.io/nvidia/tritonserver:23.09-py3"
    restart: unless-stopped
    ports:
      - 8000:8000
      - 8002:8002
    expose:
      - 8000
      - 8002
    volumes:
      - ./model_repository:/models
    environment:
      - CUDA_VISIBLE_DEVICES=""
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}  # Pass the credentials to Triton
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}  # Pass the credentials to Triton
    command: ["tritonserver", "--strict-model-config=false", "--model-control-mode=explicit", "--load-model=*", "--model-store=/models"]
    shm_size: 1g
    ulimits:
      memlock: -1
      stack: 67108864      
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.infer.rule=Host(`${INFER_DOMAIN}`)"
      - "traefik.http.services.infer.loadbalancer.server.port=8000"
      - "traefik.http.routers.infer.tls=true"
      - "traefik.http.routers.infer.entrypoints=websecure"
      - "traefik.http.routers.infer.middlewares=ip-filter@file"

  redis:
    image: redis:latest
    profiles: ["full"]
    container_name: redis  # Give it a fixed container name
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3


  redisinsight:
    image: redislabs/redisinsight:latest
    container_name: redisinsight
    profiles: ["full2"]
    ports:
      - "8001:8001"

    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.redis-gui.rule=Host(`demo.ai.qylis.com`) && PathPrefix(`/sfjdfjdfhnnfmnnjv`)"
      - "traefik.http.routers.redis-gui.entrypoints=websecure"
      - "traefik.http.services.redis-gui.loadbalancer.server.port=5000"
      - "traefik.http.routers.redis-gui.middlewares=admin-auth,ip-filter@file,redis-stripprefix"
      - "traefik.http.middlewares.admin-auth.basicauth.users=admin:$$apr1$$Jnxi624s$$FgiRkmvN8PgEwD5U/J4cX/"
      - "traefik.http.middlewares.redis-stripprefix.stripprefix.prefixes=/sfjdfjdfhnnfmnnjv"
      - "traefik.http.routers.redis-gui.tls=true"
      - "traefik.docker.network=traefik_net"
    depends_on:
      - redis

  ssd_mobilenet:
    profiles: ["full"]
    container_name: ssd_mobilenet
    image: qylisdm.gvniot.net/ssd_mobilenet:v1
    # restart: unless-stopped
    ports:
      - 8080:8080
    expose:
      - 8080
    environment:
      - REGISTRY_USER=${REGISTRY_USER}
      - REGISTRY_PASSWORD=${REGISTRY_PASSWORD}
      - PYTHONUNBUFFERED=1
      - HTTP_INFERENCE_SERVER_URL=infer:8000
      - QYLIS_REDIS_HOST=redis 
      - QYLIS_REDIS_PORT=6379
      - E_QYLIS_APP_PORT=8080
      - APP_IMAGE=testapp
      - APP_TAG=v1
      - APP_SHA=sha256:1234
      - QYLIS_APP_RESOURCE_ID=123456
      # In production this one will be passed by restsv program
    # shm_size: 1g
    # ulimits:
    #   memlock: -1
    #   stack: 67108864
    depends_on:
      - redis

  minio:
    container_name: minio
    profiles: ["full"]
    image: minio/minio
    hostname: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    expose:
      - "9001"
      - "9000"
    volumes:
      - ./minio/data:/data
      - ./minio/config:/root/.minio
    environment:
      MINIO_ROOT_USER: 'minioadmin'
      MINIO_ROOT_PASSWORD: 'minio123'
    entrypoint: /bin/sh -c
    command: -c 'minio server --console-address ":9001" /data'
    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10s

  kafka:
    container_name: kafka
    profiles: ["full", "airflow"]
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: SASL_PLAINTEXT://kafka:9092,SASL_PLAINTEXT_HOST://${KAFKA_HOST:-localhost}:9093
      KAFKA_LISTENERS: SASL_PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT_HOST://0.0.0.0:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_PLAINTEXT_HOST:SASL_PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: SASL_PLAINTEXT
      # Security settings
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SECURITY_PROTOCOL: SASL_PLAINTEXT
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf"
      # Tuning parameters for better startup
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      # Controller settings
      KAFKA_CONTROLLER_SOCKET_TIMEOUT_MS: 60000
      KAFKA_CONTROLLER_MESSAGE_QUEUE_SIZE: 10
      KAFKA_REPLICA_LAG_TIME_MAX_MS: 30000
      # Connection settings
      KAFKA_CONNECTIONS_MAX_IDLE_MS: 60000
      KAFKA_SOCKET_CONNECTION_SETUP_TIMEOUT_MS: 60000
      KAFKA_SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS: 120000
      # Allow more time for broker to start
      KAFKA_BROKER_RACK: "rack1"
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    volumes:
      - ./config/kafka_server_jaas.conf:/etc/kafka/kafka_server_jaas.conf:ro
      - ./config/client.properties:/etc/kafka/client.properties:ro
    ports:
      - "9092:9092"
      - "9093:9093"

    depends_on:
      - zookeeper
    #restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list --command-config /etc/kafka/client.properties || exit 1"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 60s  # Increased to give more time for initial setup

  zookeeper:
    container_name: zookeeper
    profiles: ["full", "airflow"]
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # SASL Configuration for Zookeeper
      ZOOKEEPER_SASL_ENABLED: "true"
      ZOOKEEPER_AUTH_PROVIDER_SASL: "org.apache.zookeeper.server.auth.SASLAuthenticationProvider"
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/zookeeper_jaas.conf"
      #ZOOKEEPER_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";'        
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT_ADDRESS: 0.0.0.0
      ZOOKEEPER_MAX_CLIENT_CNXNS: 100
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: "stat, ruok"
    volumes:
      - ./config/zookeeper_jaas.conf:/etc/kafka/zookeeper_jaas.conf:ro
    healthcheck:
      #test: ["CMD-SHELL", "timeout 5 bash -c 'echo ruok | nc localhost 2181 | grep -q imok'"]
      test: ["CMD", "bash", "-c", "echo stat | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

    ports:
      - "2181:2181"
    #restart: unless-stopped

# Observability Profile Components

  elasticsearch:
    profiles: ["full", "obs_elastic", "obs_full"]
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false

    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200 >/dev/null 2>&1 || exit 1"]
      interval: 10s
      retries: 5

  kibana:
    profiles: ["full", "obs_elastic", "obs_full"]
    image: docker.elastic.co/kibana/kibana:8.12.2
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      XPACK_SECURITY_ENABLED: "false"
    depends_on:
      - elasticsearch

    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:5601 >/dev/null 2>&1 || exit 1"]
      interval: 10s
      retries: 5

  otel-collector:
    profiles: ["full", "obs_otel", "obs_full"]
    image: otel/opentelemetry-collector-contrib:0.70.0
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./config/otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4318:4318"  # OTLP HTTP receiver
      - "4317:4317"  # OTLP gRPC receiver
      - "8889:8889"  # Prometheus metrics

    depends_on:
      - elasticsearch
        
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "localhost:13133"]
      interval: 10s
      timeout: 5s
      retries: 5

  jaeger:
    profiles: ["full", "obs_jaeger", "obs_full"]
    image: jaegertracing/all-in-one:1.50
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"  # UI
      - "4319:4318"    # OTLP HTTP
      - "4320:4317"    # OTLP gRPC


# Add these services to your docker-compose.yml

  airflow-postgres:
    profiles: ["full", "airflow", "obs_full"]
    image: postgres:17
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5

    ports:
      - "5433:5432"  # Different port from your existing postgres

  airflow-redis:
    profiles: ["full", "airflow", "obs_full"]
    image: redis:7.2-bookworm
    ports:
      - "6380:6379"  # Different port from your existing redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      retries: 50



  airflow-webserver:
    profiles: ["full", "airflow", "obs_full"]
    image: qylis_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@airflow-redis:6379/0
      AIRFLOW__METRICS__OTEL_ON: "True"
      AIRFLOW__METRICS__OTEL_HOST: otel-collector
      AIRFLOW__METRICS__OTEL_PORT: 4318
      AIRFLOW__TRACES__OTEL_ON: "True"
      AIRFLOW__TRACES__OTEL_HOST: otel-collector
      AIRFLOW__TRACES__OTEL_PORT: 4318
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: webserver
    ports:
      - "8078:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      - airflow-postgres
      - airflow-redis

    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.airflow.rule=Host(`${AIRFLOW_DOMAIN:-airflow.localhost}`)"
      - "traefik.http.services.airflow.loadbalancer.server.port=8080"
      - "traefik.http.routers.airflow.tls=true"
      - "traefik.http.routers.airflow.entrypoints=websecure"

  airflow-scheduler:
    profiles: ["full", "airflow", "obs_full"]
    image: qylis_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@airflow-redis:6379/0
      AIRFLOW__METRICS__OTEL_ON: "True"
      AIRFLOW__METRICS__OTEL_HOST: otel-collector
      AIRFLOW__METRICS__OTEL_PORT: 4318
      AIRFLOW__TRACES__OTEL_ON: "True"
      AIRFLOW__TRACES__OTEL_HOST: otel-collector
      AIRFLOW__TRACES__OTEL_PORT: 4318
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      - airflow-postgres
      - airflow-redis


  airflow-worker:
    profiles: ["full", "airflow", "obs_full"]
    image: qylis_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@airflow-redis:6379/0
      AIRFLOW__METRICS__OTEL_ON: "True"
      AIRFLOW__METRICS__OTEL_HOST: otel-collector
      AIRFLOW__METRICS__OTEL_PORT: 4318
      AIRFLOW__TRACES__OTEL_ON: "True"
      AIRFLOW__TRACES__OTEL_HOST: otel-collector
      AIRFLOW__TRACES__OTEL_PORT: 4318
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || exit 1'
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      - airflow-postgres
      - airflow-redis
        

  airflow-init:
    profiles: ["full", "airflow", "obs_full"]
    image: qylis_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@airflow-redis:6379/0
    command: |
      bash -c "
        airflow db migrate &&
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
      "
    depends_on:
      - airflow-postgres
        
 

  nats:
    container_name: nats
    profiles: ["full", "iam"]
    image: nats:latest
    command: [
      "--auth", "${NATS_AUTH_TOKEN:-aiboxsecret}",
      "--http_port", "8222",
      "--debug",
      "--trace",
      "--no_advertise"
    ]
    ports:
      - "4222:4222"  # NATS client connections
      - "8222:8222"  # HTTP monitoring
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8222/varz"]
      interval: 10s
      timeout: 5s
      retries: 5

    volumes:
      - nats-data:/nats-data  # For persistence
    environment:
      # JWT-based authentication example (you can change to TOKEN if preferred)
      # - NATS_JWT_FOR_ACCOUNT_SERVER=...
      # - NATS_ACCOUNT_SERVER_URL=...
      - NATS_CONFIG_FILE=/nats-data/nats-server.conf  # For future config file if needed
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.nats-monitor.rule=Host(`${MONITOR_DOMAIN}`) && PathPrefix(`/nats`)"
      - "traefik.http.services.nats-monitor.loadbalancer.server.port=8222"
      - "traefik.http.routers.nats-monitor.service=nats-monitor"
      - "traefik.http.routers.nats-monitor.tls=true"
      - "traefik.http.routers.nats-monitor.entrypoints=websecure"
      - "traefik.http.routers.nats-monitor.middlewares=admin-auth,ip-filter@file,nats-stripprefix"
      - "traefik.http.middlewares.nats-stripprefix.stripprefix.prefixes=/nats"

networks:
  traefik_net:
    driver: bridge
volumes:
    #elasticsearch-data:
    postgres_data:
    pgadmin-data:
    airflow-postgres-data:
    nats-data:
